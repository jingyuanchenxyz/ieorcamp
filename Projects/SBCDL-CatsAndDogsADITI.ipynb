{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dcd81251-ebd8-4dfa-b1b3-8cca70ae305e",
   "metadata": {},
   "source": [
    "# Go wild -- this assignment is to recognize Cats and Dogs. The load dataset function is loading the data properly.\n",
    "# Note that the image are in color, i.e., different from the MNIST and Fashion image. The tensors will be different\n",
    "# The rest is yours. Here are a few suggestions\n",
    "\n",
    "# 1. Start with a basic model -- that is your baseline\n",
    "# 2. Move a convolutional network\n",
    "# 3. Finish with a convolutional network with data augmentation (see the video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792ce082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE ONLY IF using a GPU on PACE\n",
    "\n",
    "# If this is not imported the GPU on the PACE will not function properly\n",
    "# otherwise, on another system, it will need built with cuda and loaded \n",
    "# the same version to work properly\n",
    "# os.system(\"module load cuda/12.1.1\")\n",
    "import os\n",
    "import time\n",
    "os.system(\"module load cuda/12.1.1\")\n",
    "time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba135fa5-1d46-4d81-9a73-6507077d1afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "import os, shutil, pathlib\n",
    "from keras.utils import load_img\n",
    "from keras.utils import img_to_array\n",
    "from keras.utils import array_to_img\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "size_picture = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f633c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import image_dataset_from_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f76a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensor_from_image(name):\n",
    "    img = load_img(name)\n",
    "    img = img.resize((size_picture,size_picture))\n",
    "    return (img,img_to_array(img).astype(\"uint8\").reshape((size_picture,size_picture,3)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947ad042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(start_index, end_index, dataset_type=\"training_set\"):\n",
    "    images = []\n",
    "    total_images = 2 * (end_index - start_index + 1)  # 2x for cats + dogs\n",
    "    tensor_inputs = np.zeros((total_images, size_picture, size_picture, 3))\n",
    "    tensor_labels = np.zeros((total_images, 1))\n",
    "    \n",
    "    # ✅ Handle folder name differences (test_set uses plural)\n",
    "    category_folder = {\n",
    "        \"cat\": \"cats\" if dataset_type == \"test_set\" else \"cat\",\n",
    "        \"dog\": \"dogs\" if dataset_type == \"test_set\" else \"dog\"\n",
    "    }\n",
    "    \n",
    "    base_directory = pathlib.Path(f'../datasets/cats_and_dogs/{dataset_type}')\n",
    "    k = 0  # Track loaded images\n",
    "    \n",
    "    for category in {\"cat\", \"dog\"}:\n",
    "        folder = category_folder[category]\n",
    "        names = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index + 1)]\n",
    "        \n",
    "        for n in names:\n",
    "            fname = base_directory / folder / n\n",
    "            img, t = tensor_from_image(fname)  # ✅ Assumes file exists\n",
    "            images.append(img)\n",
    "            tensor_inputs[k] = t\n",
    "            tensor_labels[k] = (category == 'dog')  # 1 for dog, 0 for cat\n",
    "            k += 1\n",
    "    \n",
    "    return images, tensor_inputs, tensor_labels\n",
    "\n",
    "# ✅ Training data (1-4000 from training_set)\n",
    "training_images, training_inputs, training_labels = load_dataset(1, 4000, \"training_set\")\n",
    "\n",
    "# ✅ Testing data (4001-5000 from test_set)\n",
    "testing_images, testing_inputs, testing_labels = load_dataset(4001, 5000, \"test_set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02dcfe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_classify(model,inputs,outputs):\n",
    "  callbacks_list = [\n",
    "      keras.callbacks.EarlyStopping(monitor = \"val_binary_accuracy\", patience = 2, mode = \"max\")\n",
    "  ]\n",
    "  model.compile(optimizer=\"rmsprop\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n",
    "  model.fit(inputs,outputs,epochs=10)\n",
    "  if val_data:\n",
    "      model.fit(inputs, outputs, epochs=epochs, validation_data=val_data, batch_size=32)\n",
    "  else:\n",
    "      model.fit(inputs, outputs, epochs=epochs, batch_size=32)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbd0104-1019-49ea-a069-f27cf5f84567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data (1-4000 from training_set)\n",
    " training_images, training_inputs, training_labels = load_dataset(1, 4000,\"training_set\")\n",
    " # Testing data (4001-5000 from test_set)\n",
    " testing_images, testing_inputs, testing_labels = load_dataset(4001, 5000,\"test_set\")\n",
    " size_picture = 128\n",
    " [ ]: # Define the split index\n",
    " split_index = int(0.8 * len(training_images)) # 80% for training, 20% for␣validation\n",
    " # Partition the data\n",
    " partial_train_data = train_data[10000:]\n",
    " partial_train_labels = train_labels[10000:]\n",
    " validation_data = train_data[:10000]\n",
    " validation_labels = train_labels[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebe1f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_images,training_inputs,training_labels) = load_dataset(1,3000)\n",
    "(testing_images,testing_inputs,testing_labels) = load_dataset(3001,3999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa9958c-05f5-4232-b850-7862f01e3644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_DNN():\n",
    "    input = keras.Input(shape=(180, 180, 3), name=\"cats_or_dogs\")\n",
    "    x = layers.Rescaling(1./255)(input)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation=\"relu\")(x)\n",
    "    x = layers.Dense(64, activation=\"relu\")(x)\n",
    "    output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    return keras.Model(inputs=input, outputs=output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066ba1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model2_CNN():\n",
    "    input = keras.Input(shape=(180, 180, 3), name=\"image_input\")\n",
    "    x = layers.Rescaling(1./255)(input)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    output = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs=input, outputs=output)\n",
    "return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84836160-df14-416c-be3c-59af3d3ae13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model3_augmented_CNN():\n",
    "    datagen = keras.sequential([\n",
    "        layer.RandomFlip(\"Horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layer.RandomZoom(0.2),\n",
    "    ])\n",
    "    input = keras.Input(shape=(size_picture, size_picture, 3),name = \"data\")\n",
    "    x= datagen(input)\n",
    "    x= layers.Rescaling(1/255.0)(x)\n",
    "    \n",
    "    x = layers.Conv2D(filters = 32, kernel_size = 3, activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2,2))(x)\n",
    "    x = layers.Conv2D(filters = 64, kernel_size = 3, activation='relu')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2,2))(x)\n",
    "    x = layers.Conv2D(filters = 128, kernel_size = 3, activation='relu'(x)\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    x= Dense(64,activation = \"relu\")(x)\n",
    "    x= Dense(32,activation = \"relu\")(x)\n",
    "    output = Dense(1,activation = \"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs=input, outputs=output)\n",
    "return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646eb885-db72-4438-8bb5-8fee2a154c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_model = binary_classify(build_model_DNN, train_inputs, train_labels, val_data=(val_inputs, val_labels))\n",
    "cnn_model = binary_classify(build_model2_CNN, train_inputs, train_labels, val_data=(val_inputs, val_labels))\n",
    "cnn_aug_model = binary_classify(build_model3_augmented_CNN, train_inputs, train_labels, val_data=(val_inputs, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c242126-8bdb-4187-9bf3-0478b1703c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating DNN:\")\n",
    "test_loss_dnn, test_acc_dnn = dnn_model.evaluate(test_inputs, test_labels)\n",
    "print(f\"DNN Accuracy: {test_acc_dnn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e242e8cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Evaluating CNN:\")\n",
    "test_loss_cnn, test_acc_cnn = cnn_model.evaluate(test_inputs, test_labels)\n",
    "print(f\"CNN Accuracy: {test_acc_cnn:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdefd5e5-5bf8-48ed-ad3f-c814be219067",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating Augmented CNN:\")\n",
    "test_loss_aug, test_acc_aug = cnn_aug_model.evaluate(test_inputs, test_labels)\n",
    "print(f\"Augmented CNN Accuracy: {test_acc_aug:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7b37ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e1f832",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10259b32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556aceeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sethbonder",
   "language": "python",
   "name": "sethbonder"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
